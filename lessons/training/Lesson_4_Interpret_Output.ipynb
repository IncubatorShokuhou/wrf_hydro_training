{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Interpret Output // STATUS - EDIT MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Diagnostic: \n",
    "For each iteration of the calibration all the metrics listed in Table below are calculated and a number of plots are generated to ease the assessment of the progress made at a given iteration. These plots help to check whether the calibration is heading toward the right direction and diagnose for possible bugs and disruption in the calibration process. Table below summarized all the metrics that are currently calculated as a part of calibration process. \n",
    "\n",
    "**PATRICK: INSERT TABLE HERE**\n",
    "\n",
    "After 1st iteration model (WRF-Hydro) run is complete, then a script called `calib_workflow` will do the followings:\n",
    "* Read the model simulated flows\n",
    "* Pair the model simulations with the observed streamflow\n",
    "* Calculate the error metrics and objective function\n",
    "* Generate a new parameter set for next iteration\n",
    "* Generate a series of diagnostic plots. A directory called `plots` will be populated in the `RUN.CALIB` directory with a number of plots depending on what options have been activated in the `setup.parm` file. The example in the previous lessons was using only the streamflow observation for calibration and therefore there is only plots related to the progress of parameters as well as streamflow error metrics and hydrographs. \n",
    "\n",
    "Let's take a look at few of these plots. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# List all the plots generated while calibration is progressing \n",
    "ls /home/docker/example_case/Calibration/output/example1/01447720/RUN.CALIB/plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above some of the plots are duplicated with the tag name `outlier`. This is added to the calibration workflow to remove the very large numbers (ouliers) from the plots so they are readable. Figures with the tag `_outlier` are containing all the interations and are the ones that we will check here. Let's begin with checking how the objective function is progressing with iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.CALIB/plots/01447720_calib_run_obj_outlier.png\" width=\"600\" height=\"600\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot keep tracks of how objective function defined in the `setup.parm` file is progressing with iterations. It should be noted that DDS is a minimization algorithm. If user select an objective function like KGE that the ideal value is the max value, then in the calibration code we are using 1-KGE as objective function. The same goes for different variant of NSE, correlation coefficient and lbem. A healthy calibration procedure would asymptote at the higher iterations numbers when we are getting close to the user defined number of iterations. The red start shows the iterations which has the best results so far. \n",
    "\n",
    "Next, we will take a look at how other error metrics are progressing with iterations next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.CALIB/plots/01447720_metric_calib_run_outlier.png\" width=\"600\" height=\"600\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows how different streamflow metrics are evolving with iterations. Ideally the overall trend is that with the improvement of the objective function, all other metrics improve as well. However, that is not the case always and therefore we keep an eye on the performance of the model to make sure we are not degrading other model performance aspects during calibration. \n",
    "\n",
    "**AREZOO**: Lets point out the details on categorical metrics and event based ones if not specified clearly in the table. \n",
    "\n",
    "Plot below is the equivalent plot from official NWMv21 calibration. Note that the objective function in NWMv21 was 1 - weighted NSE and log NSE while the objective function used in this exercise is 1 - KGE. As part of NWMv30 RnD, we tested several different objective functions and decided to use KGE for NWMv30 onboarding, and therefore used in this training. Also note, there is a number of metrics that are reported in NWMv30 that did not exists in NWMv21 calibration. In NWMV21 for this gage with improvements in the objective function, we also improved other metrics such correlation coefficient, KGE and etc which is the desired outcome. \n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"./images/01447720_metric_calib_run_outlier.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n",
    "\n",
    "Let's take a look at how parameters have evolved with calibration progress. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AREZOO** revisit this in case you add DDS description in lesson1. \n",
    "\n",
    "The above plot shows how parameters are changing with iterations, it is one way to make sure you are calibrating all the parameters that you flagged. Note that in the early iterations, a larger subset of parameters are being perturbed and as we reach to the end of calibration (larger iterations), fewer parameters are purtubed. It might be more obvious in plot below which is for the same gage from NWMv21. \n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"./images/01447720_parameters_calib_run_outlier.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n",
    "\n",
    "In initial iterations, the DDS algorithm searches globally and as the procedure approaches the maximum user-defined number of iterations, the search transitions from a global to a local search. This transition from a global to local search is achieved by dynamically and probabilistically reducing the search dimension which is the subset of the calibration parameters that will be updated in a given iteration. \n",
    "\n",
    "The probability of a parameter to be chosen for inclusion in the search is equal to P(i) = 1 - ln(i)/ln(m), where i is the iteration number and m is the maximum iteration number. Therefore the possibility of a parameter to be chosen reduces with increase in iteration numbers. In the initial iterations almost all the parameters will be modified and as it approaches the maximum number of iterations it will only modify a few parameters or only one. Parameters selected in each iteration are perturbed within the defined parameter range. The suggested lower and upper limits were shown in lesson 1. The limits are selected based on previous literature review and experts opinion. \n",
    "\n",
    "The maximum number of iterations used in the previous versions of NWM calibration was set to 300 except for the domains that were too large (> 5000 km2) in that case 150 iterations were used for the calibration.  \n",
    "\n",
    "Next we will take a look at streamflow hydrograph and the scatter plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.CALIB/plots/01447720_hydrograph.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.CALIB/plots/01447720_scatter.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot has only the model simulations with the default parameters (first iteration), the best parameters and the last iterations. We do not plot all iterations in order to have a cleaner picture. However, all the time series are saved in a Rdataset and could be used afterward to plot any other iterations if required. We will descript this Rdataset and how to pull info from it in a separate lesson. Below plots are the equivalent plots from NWMv21 for the same gage. \n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"./images/01447720_hydrograph.png\" width=\"600\" height=\"600\" />\n",
    "<img src=\"./images/01447720_scatter.png\" width=\"600\" height=\"600\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Diagnostics:\n",
    "\n",
    "After calibration is finished, the model runs for both the default and best parameters (from calibration step) for the full duration of (calibration/validation) and metrics are calculate for calibration, validation and full period. Finally a set of diagnostic plots are generated depending on what options user has selected in the `setup.parm` file. We have only calibrated using streamflow in the example provided in lesson 2 and 3, and therefore only streamflow plots are provided here. Let; take a look at the metrics first. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.VALID/plots/01447720_valid_metrics.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot summarizes how model performance has changed from default to best calibration iterations as well as how they perform during an independent time period (validation). Usually the performance of the validation period is not as good as the calibration period. \n",
    "\n",
    "The main task after calibration and validation workflow finishes is the classification of the calibration basin to Donor, Keep and Drop basins (definitions provided below) that will be used for the regionalization. Definition of Donor, Keep and Drop basins is as follows:\n",
    "\n",
    "* **Donor basins**: basins that the calibrated model performance is good, and the parameters are good to be transferred to other ungaged/uncalibrated locations. \n",
    "* **Keep basins**: basins where the calibration improved the model statistics, however we do not believe the parameters are good enough to be used for the ungages/uncalibrated locations. Therefore, the basin parameters will be kept for the basin itself but it will not be donated to other uncalibrated locations.\n",
    "* **Drop basins**: calibration was not beneficial, and therefore the parameters from calibration will not be kept for those basins. These basins will receive parameters from a donor basin in the regionalization process similar to uncalibrated areas. \n",
    "\n",
    "**AREZOO** Check whether we want to discuss the criteria's used for the selection of donors or not ... \n",
    "\n",
    "Below is the same plot from NWMv21. As one can see all the metrics improved compared to the default for both the calibration and validation period. \n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"./images/01447720_valid_metrics.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let s also check out the hydrograph and scatter plots for our current experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.VALID/plots/01447720_valid_hydrogr.png\" width=\"600\" height=\"600\" />\n",
    "<img src=\"../../../example_case/Calibration/output/example1/01447720/RUN.VALID/plots/01447720_valid_scatter.png\" width=\"600\" height=\"600\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the equivalent plots from NWMv21. \n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"./images/01447720_valid_hydrogr.png\" width=\"600\" height=\"600\" />\n",
    "<img src=\"./images/01447720_valid_scatter.png\" width=\"600\" height=\"600\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "**AREZOO** Add description on how things were tuned in NWMv21 .... change in paramters and how they impacted change in the hydrograph ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the DataBase\n",
    "\n",
    "All the paramters and error metrics for each iteraion of calibration and validation are stored in the data base and could be accessed using the sqlite library in python. Since our example case has only one gage, it is very easy to understand the content of the DB, let's take a look at all the tables that exists in the DB and their content. Let's begin with the Domain_Meta table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Let s read the Database and print the name of all the available tables in the database\n",
    "dat = sqlite3.connect(\"/home/docker/example_case/Calibration/output/DATABASE.db\")\n",
    "cursor = dat.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "names = cursor.fetchall()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the content of the Domain_Meta table in the DB \n",
    "_name = 'Domain_Meta'\n",
    "\n",
    "query = dat.execute(\"SELECT * From\" + \" \" + str(_name))\n",
    "cols = [column[0] for column in query.description]\n",
    "results= pd.DataFrame.from_records(data = query.fetchall(), columns = cols).transpose()\n",
    "print(_name)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided definitions of all the inputs Lesson 2, refer to the provided table for descriptions. `gage_id`, `link_id` and `domain_path` were the only required inputs from user, and the rest of the info could be left empty and some are filled out by the python workflow. Our example domain is the contributing area to USGS gage `01447720` and the ComID of link gage is located on it is `4185779`. The link_id or ComID of reach is identifier used to collect streamflow data from model outputs. `domainID` is the identifire that is used to connect this table to others and there is a unique domainID for each gage, domaiID is assdinged by the python workflow and not the user.\n",
    "\n",
    "Now let'e take a look at the `Job_Meta` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the content of the Job_Meta table in the DB \n",
    "_name = 'Job_Meta'\n",
    "\n",
    "query = dat.execute(\"SELECT * From\" + \" \" + str(_name))\n",
    "cols = [column[0] for column in query.description]\n",
    "results= pd.DataFrame.from_records(data = query.fetchall(), columns = cols).transpose()\n",
    "print(_name)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JobID is specified by the user at the time of submission of the calibration and bundles all the basins that have the same calibration properties. The rest of the information in this table comes from the `setup.parm` file that is filled by user (refer to Lesson 1). For description of all the columns refer to Lesson 2, all the info is being provided in a table. \n",
    "\n",
    "\n",
    "Now let'e take a look at the `Job_PARAM` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the content of the Job_Params table in the DB \n",
    "_name = 'Job_Params'\n",
    "\n",
    "query = dat.execute(\"SELECT * From\" + \" \" + str(_name))\n",
    "cols = [column[0] for column in query.description]\n",
    "results= pd.DataFrame.from_records(data = query.fetchall(), columns = cols).transpose()\n",
    "print(_name)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table list all the parameters that are use for calibrations with the range and default value to start from. All the basins in a given job will use the same parameters. The table was prepared and explained in Lesson 1. You could refer to Lesson 1 for definition of these parameters. Next take a look at the `Calib_Stats` table which has the calibration related information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the content of the Calib_Stats table in the DB \n",
    "_name = 'Calib_Stats'\n",
    "\n",
    "query = dat.execute(\"SELECT * From\" + \" \" + str(_name))\n",
    "cols = [column[0] for column in query.description]\n",
    "results= pd.DataFrame.from_records(data = query.fetchall(), columns = cols).transpose()\n",
    "print(_name)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above table sumamrizes error metrics for each gage (specified with the domianID) at every iterations during calibration. Parameters for each iterations are stored in a different table called `Calib_Params`, let's check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the content of the Calib_Params table in the DB \n",
    "_name = 'Calib_Params'\n",
    "\n",
    "query = dat.execute(\"SELECT * From\" + \" \" + str(_name))\n",
    "cols = [column[0] for column in query.description]\n",
    "results= pd.DataFrame.from_records(data = query.fetchall(), columns = cols)\n",
    "print(_name)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User could define which iterations is best performing from the `Calib_Stats` and find the correponding parameter values in `Calib_Params` table. Finally the last relevant table is `Valid_Stats` which summarizes the error metrics at the calibration, validation and full period for model runs with the default and best parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the content of the Valid_Stats table in the DB \n",
    "_name = 'Valid_Stats'\n",
    "\n",
    "query = dat.execute(\"SELECT * From\" + \" \" + str(_name))\n",
    "cols = [column[0] for column in query.description]\n",
    "results= pd.DataFrame.from_records(data = query.fetchall(), columns = cols).transpose()\n",
    "print(_name)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Simulations` states whether the model run is with the default or calibrated (best) parameters. `evalPeriod` indicate what the period of evaluations is. The options are `calib` which is the calibration period, `valid` is validation period and `full` is the period (continuous) that has both calibration and validation in it. Note the claibration and validation period do not have to be following each other, there could be a gap between the two or validation could happen at the years prior to calibration. \n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "We reviewed the plots that are generated during calibration and after validation and described the type of the information they provide. Database content also was investigated and we reviewed the content of DB. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
